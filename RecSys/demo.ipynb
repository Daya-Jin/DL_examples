{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.sys.path.append(os.path.dirname(os.path.abspath('.')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800167, 7) (800167,)\n",
      "(200042, 7) (200042,)\n"
     ]
    }
   ],
   "source": [
    "from dataset.dataset import load_ml\n",
    "\n",
    "batch_size = 512\n",
    "train_data, test_data = load_ml(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型概述"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 用户部分\n",
    "u_id_size = len(np.unique(train_data.u_id))\n",
    "u_id_emb_size = 32\n",
    "u_occu_size = len(np.unique(train_data.u_occu))\n",
    "u_occu_emb_size = 8\n",
    "u_agegen_size = len(np.unique(train_data.u_age_gender))\n",
    "u_agegen_emb_size = 8\n",
    "\n",
    "# 电影部分\n",
    "m_id_size = len(np.unique(train_data.m_id))\n",
    "m_id_emb_size = 32\n",
    "m_voc_size = 4308\n",
    "m_tit_emb_size = 16\n",
    "m_gen_size = 19\n",
    "m_gen_emb_size = 8\n",
    "m_year_size = 81\n",
    "\n",
    "m_tit_size = len(train_data.m_title[0])\n",
    "m_gens_size = len(train_data.m_genres[0])\n",
    "\n",
    "# 文本卷积网络参数\n",
    "n_txt_cnn_kernels = (8, 4, 2)\n",
    "text_cnn_ksize = (1, 2, 3)\n",
    "\n",
    "# FC层\n",
    "unit_fc1 = 1    # 电影年份是一个单独标量，同样经过一层FC\n",
    "unit_fc2 = 16    # 各embedding特征经过的FC层\n",
    "unit_fc3 = 128    # 单端特征的联合FC\n",
    "unit_O = 1    # 输出一个分数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 搭建模型\n",
    "首先是输入："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "u_id = tf.placeholder(tf.int32, [None], name='u_id')\n",
    "u_agegen = tf.placeholder(tf.int32, [None], name='u_agegen')\n",
    "u_occu = tf.placeholder(tf.int32, [None], name='u_occu')\n",
    "\n",
    "m_id = tf.placeholder(tf.int32, [None], name='m_id')\n",
    "m_tit = tf.placeholder(tf.int32, [None, m_tit_size], name='m_tit')\n",
    "m_gen = tf.placeholder(tf.int32, [None, m_gens_size], name='n_gen')\n",
    "m_year = tf.placeholder(tf.int32, [None], name='m_year')\n",
    "\n",
    "Y = tf.placeholder(tf.float32, [None, 1], name='rating')\n",
    "\n",
    "is_training = tf.placeholder(tf.bool)    # 训练标识位"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后是嵌入部分："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('user_embedding', initializer=tf.random_uniform_initializer(-1.0, 1.0)):\n",
    "    # u_id嵌入\n",
    "    uid_emb_lookup = tf.get_variable('uid_embedding', [u_id_size, u_id_emb_size],\n",
    "                                     dtype=tf.float32)\n",
    "    uid_emb = tf.nn.embedding_lookup(uid_emb_lookup, u_id)\n",
    "\n",
    "    # u_occu嵌入\n",
    "    uoccu_emb_lookup = tf.get_variable('uoccu_embedding', [u_occu_size, u_occu_emb_size],\n",
    "                                       dtype=tf.float32)\n",
    "    uoccu_emb = tf.nn.embedding_lookup(uoccu_emb_lookup, u_occu)\n",
    "\n",
    "    # u_age_gender嵌入\n",
    "    uagegen_emb = tf.get_variable('uagegen_embedding', [u_agegen_size, u_agegen_emb_size],\n",
    "                                  dtype=tf.float32)\n",
    "    uagegen_emb = tf.nn.embedding_lookup(uagegen_emb, u_agegen)\n",
    "\n",
    "with tf.variable_scope('movie_embedding', initializer=tf.random_uniform_initializer(-1.0, 1.0)):\n",
    "    mid_emb_lookup = tf.get_variable('mid_embedding', [m_id_size, m_id_emb_size],\n",
    "                                     dtype=tf.float32)\n",
    "    mid_emb = tf.nn.embedding_lookup(mid_emb_lookup, m_id)\n",
    "\n",
    "    mtit_emb_lookup = tf.get_variable('mtit_embedding', [m_voc_size, m_tit_emb_size],\n",
    "                                      dtype=tf.float32)\n",
    "    mtit_emb = tf.nn.embedding_lookup(mtit_emb_lookup, m_tit)\n",
    "\n",
    "    mgen_emb_lookup = tf.get_variable('mgen_embedding', [m_gen_size, m_gen_emb_size],\n",
    "                                      dtype=tf.float32)\n",
    "    mgen_emb = tf.nn.embedding_lookup(mgen_emb_lookup, m_gen)\n",
    "    mgen_emb = tf.reduce_mean(mgen_emb, axis=1)    # 查找得到的多重emb做平均"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对电影标题应用TextCNN："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-439eb88275f8>:10: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n",
      "WARNING:tensorflow:From <ipython-input-6-439eb88275f8>:12: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.max_pooling2d instead.\n",
      "WARNING:tensorflow:From <ipython-input-6-439eb88275f8>:16: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From <ipython-input-6-439eb88275f8>:17: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('TextCNN'):\n",
    "    # 在最后增加一维，扩成四维向量(batch_size,tit_len,m_tit_emb_size,1)\n",
    "    mtit_emb_exp = tf.expand_dims(mtit_emb, -1)\n",
    "\n",
    "    layers = list()\n",
    "    for i in range(len(n_txt_cnn_kernels)):\n",
    "        conv = tf.layers.conv2d(mtit_emb_exp, filters=n_txt_cnn_kernels[i],\n",
    "                                kernel_size=(text_cnn_ksize[i],\n",
    "                                             m_tit_emb_size),\n",
    "                                padding='same', activation=tf.nn.relu)\n",
    "        pool = tf.layers.max_pooling2d(conv, pool_size=(2, 1),\n",
    "                                       strides=(1, 1))\n",
    "        layers.append(pool)\n",
    "\n",
    "    tit_pool = tf.concat(layers, axis=3)\n",
    "    tit_dropout = tf.layers.dropout(tf.layers.flatten(tit_pool),\n",
    "                                    rate=0.2, training=is_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来是全连接层，同样分为user与movie两部分："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-10c0bda93fe5>:3: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('user_fc'):\n",
    "    uid_fc = tf.layers.dense(uid_emb, unit_fc2, activation=tf.nn.relu,\n",
    "                             name='uid_fc')\n",
    "    uoccu_fc = tf.layers.dense(uoccu_emb, unit_fc2, activation=tf.nn.relu,\n",
    "                               name='uoccu_fc')\n",
    "    uagegen_fc = tf.layers.dense(uagegen_emb, unit_fc2, activation=tf.nn.relu,\n",
    "                                 name='uagegen_fc')\n",
    "\n",
    "    user_fc = tf.concat([uid_fc, uoccu_fc, uagegen_fc], axis=1)\n",
    "    user_fc = tf.layers.dense(user_fc, unit_fc3, activation=tf.nn.relu)\n",
    "    user_fc = tf.layers.dropout(user_fc, rate=0.3, training=is_training,\n",
    "                                name='user_fc')\n",
    "\n",
    "with tf.name_scope('movie_fc'):\n",
    "    mid_fc = tf.layers.dense(mid_emb, unit_fc2, activation=tf.nn.relu)\n",
    "    mgen_fc = tf.layers.dense(mgen_emb, unit_fc2, activation=tf.nn.relu)\n",
    "    mtit_fc = tf.layers.dense(tit_dropout, unit_fc2, activation=tf.nn.relu)\n",
    "    myear_fc = tf.layers.dense(tf.reshape(m_year, [-1, 1]), unit_fc1,\n",
    "                               activation=tf.nn.relu) \n",
    "    myear_fc=tf.cast(myear_fc,dtype=tf.float32)    # 为了concat转换类型\n",
    "\n",
    "    movie_fc = tf.concat([mid_fc,mgen_fc,mtit_fc,myear_fc],axis=1)\n",
    "    movie_fc=tf.layers.dense(movie_fc,unit_fc3,activation=tf.nn.relu)\n",
    "    movie_fc=tf.layers.dropout(movie_fc,rate=0.3,training=is_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后后就是输出了："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = tf.expand_dims(tf.reduce_sum(user_fc * movie_fc,\n",
    "                                      axis=1), axis=1)    # 输出分数，把向量扩成矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/losses/losses_impl.py:667: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('Eval'):\n",
    "    loss = tf.losses.mean_squared_error(labels=Y, predictions=logits)\n",
    "\n",
    "with tf.name_scope('train_op'):\n",
    "    lr = 1e-3\n",
    "    train_op = tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True    # 按需使用显存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练网络\n",
    "在送入数据的时候有一点麻烦，主要是数据特征存在多重嵌套。需要把对应的列提取出来然后转化成```ndarray```送入模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_feed(x_batch, y_batch):\n",
    "    '''\n",
    "    构造feed_dict\n",
    "    '''\n",
    "    feed_dict = dict()\n",
    "    feed_dict[u_id] = x_batch[:, 0]\n",
    "    feed_dict[u_agegen] = np.asarray(x_batch[:, 1], dtype=np.int32)\n",
    "    feed_dict[u_occu] = np.asarray(x_batch[:, 2], dtype=np.int32)\n",
    "    feed_dict[m_id] = np.asarray(x_batch[:, 3], dtype=np.int32)\n",
    "\n",
    "    mtit_feed = np.zeros((len(x_batch), m_tit_size))\n",
    "    for i in range(len(x_batch)):\n",
    "        mtit_feed[i] = x_batch[i, 4]\n",
    "    feed_dict[m_tit] = np.asarray(mtit_feed, dtype=np.int32)\n",
    "\n",
    "    mgen_feed = np.zeros((len(x_batch), m_gens_size))\n",
    "    for i in range(len(x_batch)):\n",
    "        mgen_feed[i] = x_batch[i, 5]\n",
    "    feed_dict[m_gen] = np.asarray(mgen_feed, dtype=np.int32)\n",
    "\n",
    "    feed_dict[m_year] = x_batch[:, 6]\n",
    "\n",
    "    feed_dict[Y] = y_batch.reshape((-1,1))\n",
    "    \n",
    "    return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, batch_loss: 1.12532639503479\n",
      "epoch: 1, batch_loss: 0.9347590208053589\n",
      "epoch: 1, batch_loss: 1.0483558177947998\n",
      "epoch: 2, batch_loss: 1.0349704027175903\n",
      "epoch: 3, batch_loss: 0.9728089570999146\n",
      "epoch: 3, test_MSE: 0.8450213074684143\n",
      "epoch: 3, batch_loss: 1.0153937339782715\n",
      "epoch: 4, batch_loss: 0.8476366996765137\n",
      "epoch: 5, batch_loss: 0.949594259262085\n",
      "epoch: 5, batch_loss: 0.9386029243469238\n",
      "epoch: 6, batch_loss: 1.0169233083724976\n",
      "epoch: 6, test_MSE: 0.8372696042060852\n",
      "epoch: 7, batch_loss: 0.8411692380905151\n",
      "epoch: 7, batch_loss: 0.8545806407928467\n",
      "epoch: 8, batch_loss: 1.0171763896942139\n",
      "epoch: 8, batch_loss: 0.9544245600700378\n",
      "epoch: 9, batch_loss: 0.9274780750274658\n",
      "epoch: 9, test_MSE: 0.8183063268661499\n",
      "epoch: 10, batch_loss: 1.064489722251892\n",
      "epoch: 10, batch_loss: 0.9292706251144409\n",
      "epoch: 11, batch_loss: 0.9608153104782104\n",
      "epoch: 12, batch_loss: 0.9874977469444275\n",
      "epoch: 12, batch_loss: 1.015494704246521\n",
      "epoch: 12, test_MSE: 0.8124629855155945\n",
      "epoch: 13, batch_loss: 1.025295615196228\n",
      "epoch: 14, batch_loss: 0.863553524017334\n",
      "epoch: 14, batch_loss: 0.9597355127334595\n",
      "epoch: 15, batch_loss: 0.9926642179489136\n",
      "epoch: 15, batch_loss: 0.879802942276001\n",
      "epoch: 15, test_MSE: 0.8057191371917725\n",
      "epoch: 16, batch_loss: 1.004819393157959\n",
      "epoch: 17, batch_loss: 0.9190146327018738\n",
      "epoch: 17, batch_loss: 0.902825117111206\n",
      "epoch: 18, batch_loss: 0.8694093227386475\n",
      "epoch: 19, batch_loss: 0.9438643455505371\n",
      "epoch: 19, test_MSE: 0.8028337955474854\n",
      "epoch: 19, batch_loss: 0.8052991032600403\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(init)\n",
    "    epochs = 20\n",
    "\n",
    "    batch_cnt = 0\n",
    "    for epoch in range(epochs):\n",
    "        for batch_data, batch_labels in train_data.next_batch():\n",
    "            batch_cnt += 1\n",
    "            feed_dict = gen_feed(batch_data, batch_labels)\n",
    "            feed_dict[is_training]=True\n",
    "            loss_val, acc_val = sess.run(\n",
    "                [loss, train_op], feed_dict=feed_dict)\n",
    "\n",
    "            # 每1000batch输出一次信息\n",
    "            if (batch_cnt+1) % 1000 == 0:\n",
    "                print('epoch: {}, batch_loss: {}'.format(\n",
    "                    epoch, loss_val))\n",
    "\n",
    "            # 每5000batch做一次验证\n",
    "            if (batch_cnt+1) % 5000 == 0:\n",
    "                all_test_acc_val = list()\n",
    "                for test_batch_data, test_batch_labels in test_data.next_batch():\n",
    "                    feed_dict = gen_feed(test_batch_data, test_batch_labels)\n",
    "                    feed_dict[is_training]=False\n",
    "                    test_acc_val = sess.run([loss], feed_dict=feed_dict)\n",
    "                    all_test_acc_val.append(test_acc_val)\n",
    "                test_acc = np.mean(all_test_acc_val)\n",
    "                print('epoch: {}, test_MSE: {}'.format(epoch, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
